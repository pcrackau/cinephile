{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53c6abcc",
   "metadata": {},
   "source": [
    "## Try 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa86776b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "import splitfolders \n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "add7c02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = pathlib.Path(\"../images\")\n",
    "DATA_DIR = pathlib.Path(\"../finetune_data\")\n",
    "train_path = DATA_DIR / \"output/train\"\n",
    "val_path = DATA_DIR / \"output/val\"\n",
    "test_path = DATA_DIR / \"output/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4eaffdb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Images 456\n"
     ]
    }
   ],
   "source": [
    "Total_Images = glob.glob(f'{image_path}/*/*.jpg')\n",
    "print(\"Total Number of Images\", len(Total_Images))\n",
    "Total_Images = pd.Series(Total_Images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d531923b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FileName</th>\n",
       "      <th>ClassId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../images/extreme-close-up/casino.jpg</td>\n",
       "      <td>extreme-close-up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../images/extreme-close-up/psycho.jpg</td>\n",
       "      <td>extreme-close-up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../images/extreme-close-up/savingprivateryan.jpg</td>\n",
       "      <td>extreme-close-up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../images/extreme-close-up/capricorn-one.jpg</td>\n",
       "      <td>extreme-close-up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../images/extreme-close-up/goldfinger.jpg</td>\n",
       "      <td>extreme-close-up</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           FileName           ClassId\n",
       "0             ../images/extreme-close-up/casino.jpg  extreme-close-up\n",
       "1             ../images/extreme-close-up/psycho.jpg  extreme-close-up\n",
       "2  ../images/extreme-close-up/savingprivateryan.jpg  extreme-close-up\n",
       "3      ../images/extreme-close-up/capricorn-one.jpg  extreme-close-up\n",
       "4         ../images/extreme-close-up/goldfinger.jpg  extreme-close-up"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample visualization \n",
    "\n",
    "Total_Df = pd.DataFrame()\n",
    "\n",
    "Total_Df['FileName'] = Total_Images.map(lambda ImageName :ImageName.split(\"H\")[-1])\n",
    "\n",
    "Total_Df['ClassId'] = Total_Images.map(lambda ImageName :ImageName.split(\"/\")[-2])\n",
    "\n",
    "Total_Df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "26399186",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ClassId\n",
       "close-up            145\n",
       "medium-shot         140\n",
       "long-shot           110\n",
       "medium-long-shot     44\n",
       "extreme-close-up     17\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# image counts \n",
    "\n",
    "Class_Id_Dist_Total = Total_Df['ClassId'].value_counts()\n",
    "Class_Id_Dist_Total.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "92de66da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying files: 456 files [00:00, 5373.81 files/s]\n"
     ]
    }
   ],
   "source": [
    "splitfolders.ratio(image_path, output=f\"{DATA_DIR}/output\", seed=101, ratio=(.8, .1, .1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "31b071ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path=f'{DATA_DIR}/output/train/'\n",
    "val_path=f'{DATA_DIR}/output/val'\n",
    "test_path=f'{DATA_DIR}/output/test'\n",
    "class_names=os.listdir(train_path)\n",
    "class_names_val=os.listdir(val_path)\n",
    "class_names_test=os.listdir(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "99b37d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of training images:  364\n",
      "Total number of test images:  48\n",
      "Total number of val images:  44\n"
     ]
    }
   ],
   "source": [
    "train_image1 = glob.glob(f'{DATA_DIR}/output/train/*/*.jpg')\n",
    "\n",
    "Total_TrainImages = train_image1 \n",
    "print(\"Total number of training images: \", len(Total_TrainImages))\n",
    "\n",
    "\n",
    "test_image1 = glob.glob(f'{DATA_DIR}/output/test/*/*.jpg')\n",
    "\n",
    "Total_TestImages = test_image1\n",
    "print(\"Total number of test images: \", len(Total_TestImages))\n",
    "\n",
    "\n",
    "\n",
    "Val_image1 = glob.glob(f'{DATA_DIR}/output/val/*/*.jpg')\n",
    "\n",
    "Total_ValImages = Val_image1 \n",
    "print(\"Total number of val images: \", len(Total_ValImages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "131ae4e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['extreme-close-up', 'medium-shot', 'medium-long-shot', 'long-shot', 'close-up']\n"
     ]
    }
   ],
   "source": [
    "class_names = os.listdir(train_path)\n",
    "print(\"Classes:\", class_names)\n",
    "\n",
    "# Load datasets\n",
    "IMG_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "12b180a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 364 files belonging to 5 classes.\n",
      "Found 44 files belonging to 5 classes.\n",
      "Found 48 files belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "train_ds = image_dataset_from_directory(\n",
    "    train_path,\n",
    "    image_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    label_mode=\"categorical\"\n",
    ")\n",
    "\n",
    "val_ds = image_dataset_from_directory(\n",
    "    val_path,\n",
    "    image_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    label_mode=\"categorical\"\n",
    ")\n",
    "\n",
    "test_ds = image_dataset_from_directory(\n",
    "    test_path,\n",
    "    image_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    label_mode=\"categorical\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91999c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## OPTIONAL: preprocessing: crop images to better represent aspect ratio\n",
    "\n",
    "def crop_to_aspect(image, aspect_ratio=1.5):\n",
    "    shape = tf.shape(image)\n",
    "    h, w = shape[0], shape[1]\n",
    "    target_w = tf.cast(h * aspect_ratio, tf.int32)\n",
    "    target_h = tf.cast(w / aspect_ratio, tf.int32)\n",
    "\n",
    "    # If too wide, crop width\n",
    "    if w > target_w:\n",
    "        offset_w = (w - target_w) // 2\n",
    "        image = tf.image.crop_to_bounding_box(image, 0, offset_w, h, target_w)\n",
    "    else:  # Else, crop height\n",
    "        offset_h = (h - target_h) // 2\n",
    "        image = tf.image.crop_to_bounding_box(image, offset_h, 0, target_h, w)\n",
    "    return image\n",
    "\n",
    "def preprocess(image, label):\n",
    "    image = crop_to_aspect(image, aspect_ratio=1.5)\n",
    "    image = tf.image.resize(image, [224, 224])  # Or 299, etc.\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    return image, label\n",
    "\n",
    "train_ds = train_ds.map(preprocess)\n",
    "val_ds = val_ds.map(preprocess)\n",
    "test_ds = test_ds.map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7da670b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 633ms/step - accuracy: 0.3454 - loss: 2.5709 - val_accuracy: 0.5455 - val_loss: 1.3048\n",
      "Epoch 2/10\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 560ms/step - accuracy: 0.6555 - loss: 1.1177 - val_accuracy: 0.6364 - val_loss: 1.0796\n",
      "Epoch 3/10\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 553ms/step - accuracy: 0.7594 - loss: 0.7021 - val_accuracy: 0.5682 - val_loss: 1.0550\n",
      "Epoch 4/10\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 554ms/step - accuracy: 0.7255 - loss: 0.7149 - val_accuracy: 0.5227 - val_loss: 1.2310\n",
      "Epoch 5/10\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 552ms/step - accuracy: 0.8300 - loss: 0.4827 - val_accuracy: 0.6591 - val_loss: 1.0848\n",
      "Epoch 6/10\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 552ms/step - accuracy: 0.8293 - loss: 0.4737 - val_accuracy: 0.5682 - val_loss: 1.2195\n",
      "Epoch 7/10\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 556ms/step - accuracy: 0.8884 - loss: 0.3574 - val_accuracy: 0.5909 - val_loss: 1.2131\n",
      "Epoch 8/10\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 553ms/step - accuracy: 0.9323 - loss: 0.2503 - val_accuracy: 0.5909 - val_loss: 1.2249\n",
      "Epoch 9/10\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 567ms/step - accuracy: 0.9041 - loss: 0.2611 - val_accuracy: 0.5909 - val_loss: 1.3110\n",
      "Epoch 10/10\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 586ms/step - accuracy: 0.9386 - loss: 0.2024 - val_accuracy: 0.6364 - val_loss: 1.2034\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x15b204750>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "train_ds = train_ds.prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.prefetch(buffer_size=AUTOTUNE)\n",
    "test_ds = test_ds.prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "# Build model with ResNet50 base\n",
    "base_model = ResNet50(\n",
    "    weights='imagenet',\n",
    "    include_top=False,\n",
    "    input_shape=(224, 224, 3)\n",
    ")\n",
    "base_model.trainable = False  # Freeze base\n",
    "\n",
    "model = models.Sequential([\n",
    "    base_model,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(len(class_names), activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_ds, validation_data=val_ds, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ac1b5d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 292ms/step - accuracy: 0.5278 - loss: 1.0595\n",
      "Test accuracy: 0.54\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_ds)\n",
    "print(f\"Test accuracy: {test_acc:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cine",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
